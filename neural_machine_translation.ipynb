{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-machine-translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI6YvwuyVmNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This tutorial is to Develop a Deep Learning Model to Automatically\n",
        "# Translate from German to English in Python with Keras, Step-by-Step.\n",
        "# Machine translation is a challenging task that traditionally involves large statistical models \n",
        "# developed using highly sophisticated linguistic knowledge.\n",
        "\n",
        "# Neural machine translation is the use of deep neural networks for the problem of machine translation.\n",
        "\n",
        "# In this tutorial, you will discover how to develop a neural machine translation system for translating German phrases to English.\n",
        "\n",
        "# After completing this tutorial, you will know:\n",
        "\n",
        "# How to clean and prepare data ready to train a neural machine translation system.\n",
        "# How to develop an encoder-decoder model for machine translation.\n",
        "# How to use a trained model for inference on new input phrases and evaluate the model skill.\n",
        "\n",
        "# Tutorial Overview\n",
        "# This tutorial is divided into 4 parts; they are:\n",
        "\n",
        "# German to English Translation Dataset\n",
        "# Preparing the Text Data\n",
        "# Train Neural Translation Model\n",
        "# Evaluate Neural Translation Model\n",
        "# Python Environment\n",
        "# This tutorial assumes you have a Python 3 SciPy environment installed.\n",
        "\n",
        "# You must have Keras (2.0 or higher) installed with either the TensorFlow or Theano backend.\n",
        "\n",
        "# The tutorial also assumes you have NumPy and Matplotlib installed.\n",
        "\n",
        "# If you need help with your environment, see this post:\n",
        "\n",
        "# How to Setup a Python Environment for Machine Learning and Deep Learning with Anaconda\n",
        "\n",
        "# https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/\n",
        "\n",
        "# A GPU is not require for thus tutorial, nevertheless, you can access GPUs cheaply on Amazon Web Services. Learn how in this tutorial:\n",
        "\n",
        "# https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/\n",
        "\n",
        "# How to Setup Amazon AWS EC2 GPUs to Train Keras Deep Learning Models (step-by-step)\n",
        "# Let’s dive in.\n",
        "\n",
        "# German to English Translation Dataset\n",
        "# In this tutorial, we will use a dataset of German to English terms used as the basis for flashcards for language learning.\n",
        "\n",
        "# The dataset is available from the ManyThings.org website, with examples drawn from the Tatoeba Project. The dataset is comprised of German phrases and their English counterparts and is intended to be used with the Anki flashcard software.\n",
        "\n",
        "# The page provides a list of many language pairs, and I encourage you to explore other languages:\n",
        "\n",
        "# Tab-delimited Bilingual Sentence Pairs\n",
        "# The dataset we will use in this tutorial is available for download here:\n",
        "\n",
        "# German – English deu-eng.zip\n",
        "# http://www.manythings.org/anki/deu-eng.zip\n",
        "\n",
        "# Download the dataset to your current working directory and decompress; for example:\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsFL6NMpxeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d17ce8bc-edca-4fa9-9668-885c125ce31d"
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-02 12:34:48--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4541707 (4.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "\rdeu-eng.zip           0%[                    ]       0  --.-KB/s               \rdeu-eng.zip          19%[==>                 ] 881.64K  3.93MB/s               \rdeu-eng.zip         100%[===================>]   4.33M  14.8MB/s    in 0.3s    \n",
            "\n",
            "2019-10-02 12:34:49 (14.8 MB/s) - ‘deu-eng.zip’ saved [4541707/4541707]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub6_y7lArRwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc03c08d-dd03-423c-9078-8b9bb55a554c"
      },
      "source": [
        "!unzip deu-eng.zip # remeber to open a notebook in google colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeGn-6ldrfpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You will have a file called deu.txt that contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language.\n",
        "\n",
        "# For example, the first 5 lines of the file look as follows:\n",
        "\n",
        "# Hi. Hallo!\n",
        "# Hi. Grüß Gott!\n",
        "# Run!    Lauf!\n",
        "# Wow!    Potzdonner!\n",
        "# Wow!    Donnerwetter!\n",
        "# 1\n",
        "# 2\n",
        "# 3\n",
        "# 4\n",
        "# 5\n",
        "# Hi. Hallo!\n",
        "# Hi. Grüß Gott!\n",
        "# Run!    Lauf!\n",
        "# Wow!    Potzdonner!\n",
        "# Wow!    Donnerwetter!\n",
        "\n",
        "\n",
        "# We will frame the prediction problem as given a sequence of words in German as input, translate or predict the sequence of words in English.\n",
        "\n",
        "# The model we will develop will be suitable for some beginner German phrases.\n",
        "\n",
        "# Preparing the Text Data\n",
        "# The next step is to prepare the text data ready for modeling.\n",
        "\n",
        "# If you are new to cleaning text data, see this post:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HivtJRmse7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How to Clean Text for Machine Learning with Python\n",
        "# https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
        "\n",
        "# Take a look at the raw data and note what you see that we might need to handle in a data cleaning operation.\n",
        "\n",
        "# For example, here are some observations I note from reviewing the raw data:\n",
        "\n",
        "# There is punctuation.\n",
        "# The text contains uppercase and lowercase.\n",
        "# There are special characters in the German.\n",
        "# There are duplicate phrases in English with different translations in German.\n",
        "# The file is ordered by sentence length with very long sentences toward the end of the file.\n",
        "# Did you note anything else that could be important?\n",
        "# Let me know in the comments below.\n",
        "\n",
        "# A good text cleaning procedure may handle some or all of these observations.\n",
        "\n",
        "# Data preparation is divided into two subsections:\n",
        "\n",
        "# Clean Text\n",
        "# Split Text\n",
        "# 1. Clean Text\n",
        "# First, we must load the data in a way that preserves the Unicode German characters. The function below called load_doc() will load the file as a blob of text.\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIzMfqV0Z_AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each line contains a single pair of phrases, \n",
        "# first English and then German, separated by a tab character.\n",
        "# We must split the loaded text by line and then by phrase.\n",
        "# The function to_pairs() below will split the loaded text.\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njz8GBw6aDBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are now ready to clean each sentence. The specific cleaning operations we will perform are as follows:\n",
        "\n",
        "# Remove all non-printable characters.\n",
        "# Remove all punctuation characters.\n",
        "# Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
        "# Normalize the case to lowercase.\n",
        "# Remove any remaining tokens that are not alphabetic.\n",
        "# We will perform these operations on each phrase for each pair in the loaded dataset.\n",
        "\n",
        "# The clean_pairs() function below implements these operations.\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fii1OILAeIzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g0gLmHxeJI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d731742d-a87a-4193-db11-c985bec9abb4"
      },
      "source": [
        "# Finally, now that the data has been cleaned,\n",
        "# we can save the list of phrase pairs to a file ready for use.\n",
        "\n",
        "# The function save_clean_data() uses the pickle API to \n",
        "# save the list of clean text to file.\n",
        "\n",
        "# Pulling all of this together, the complete example is listed below.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        " \n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        " \n",
        "# load dataset\n",
        "filename = 'deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-german.pkl\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stop] => [stopp]\n",
            "[wait] => [warte]\n",
            "[go on] => [mach weiter]\n",
            "[hello] => [hallo]\n",
            "[i ran] => [ich rannte]\n",
            "[i see] => [ich verstehe]\n",
            "[i see] => [aha]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[smile] => [lacheln]\n",
            "[cheers] => [zum wohl]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[got it] => [kapiert]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[i paid] => [ich habe bezahlt]\n",
            "[i paid] => [ich zahlte]\n",
            "[i sang] => [ich sang]\n",
            "[i swim] => [ich schwimme]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[im up] => [ich bin wach]\n",
            "[im up] => [ich bin auf]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das kommt nicht in frage]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n",
            "[really] => [im ernst]\n",
            "[thanks] => [danke]\n",
            "[try it] => [versuchs]\n",
            "[we won] => [wir haben gewonnen]\n",
            "[why me] => [warum ich]\n",
            "[ask tom] => [frag tom]\n",
            "[ask tom] => [fragen sie tom]\n",
            "[ask tom] => [fragt tom]\n",
            "[awesome] => [fantastisch]\n",
            "[be cool] => [entspann dich]\n",
            "[be fair] => [sei nicht ungerecht]\n",
            "[be fair] => [sei fair]\n",
            "[be nice] => [sei nett]\n",
            "[be nice] => [seien sie nett]\n",
            "[beat it] => [geh weg]\n",
            "[beat it] => [hau ab]\n",
            "[beat it] => [verschwinde]\n",
            "[beat it] => [verdufte]\n",
            "[beat it] => [mach dich fort]\n",
            "[beat it] => [zieh leine]\n",
            "[beat it] => [mach dich vom acker]\n",
            "[beat it] => [verzieh dich]\n",
            "[beat it] => [verkrumele dich]\n",
            "[beat it] => [troll dich]\n",
            "[beat it] => [zisch ab]\n",
            "[beat it] => [pack dich]\n",
            "[beat it] => [mach ne fliege]\n",
            "[beat it] => [schwirr ab]\n",
            "[beat it] => [mach die sause]\n",
            "[beat it] => [scher dich weg]\n",
            "[beat it] => [scher dich fort]\n",
            "[call me] => [ruf mich an]\n",
            "[come in] => [komm herein]\n",
            "[come in] => [herein]\n",
            "[come on] => [komm]\n",
            "[come on] => [kommt]\n",
            "[come on] => [mach schon]\n",
            "[come on] => [macht schon]\n",
            "[come on] => [komm schon]\n",
            "[get tom] => [hol tom]\n",
            "[get out] => [raus]\n",
            "[get out] => [geht raus]\n",
            "[get out] => [geh raus]\n",
            "[get out] => [geht raus]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTwCZpEJhdEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}